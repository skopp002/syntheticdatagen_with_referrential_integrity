{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Current idle_timeout is 2880 minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\nPrevious number of workers: 5\nSetting new number of workers to: 5\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nIdle Timeout: 2880\nSession ID: b081ca53-d230-487a-b0f4-78def75a79df\nApplying the following default arguments:\n--glue_kernel_version 1.0.5\n--enable-glue-datacatalog true\nWaiting for session b081ca53-d230-487a-b0f4-78def75a79df to get into ready status...\nSession b081ca53-d230-487a-b0f4-78def75a79df has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "dyf = glueContext.create_dynamic_frame.from_catalog(database='syntheticdata', table_name='employee')\ndyf.printSchema()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- employee_id: string\n|-- dept_id: string\n|-- employee_level: string\n|-- employment_start_dt: string\n|-- increment_dt: string\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Convert the DynamicFrame to a Spark DataFrame and display a sample of the data\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "df = dyf.toDF()\ndf.show()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------+-------+--------------+-------------------+------------+\n|employee_id|dept_id|employee_level|employment_start_dt|increment_dt|\n+-----------+-------+--------------+-------------------+------------+\n|    X110096|   4086|          null|           20240924|        null|\n|    X110088|   4078|          null|           20240924|        null|\n|    X110011|   4001|          null|           20240924|        null|\n|    X110037|   4027|          null|           20240924|        null|\n|    X110062|   4052|          null|           20240924|        null|\n|    X114535|   4521|          null|           20240925|        null|\n|    X115566|   4551|          null|           20240925|        null|\n|    X118353|   4335|          null|           20240925|        null|\n|    X110548|   4538|          null|           20240925|        null|\n|    X112198|   4186|          null|           20240925|        null|\n|    X116149|   4133|          null|           20240925|        null|\n|    X116800|   4784|          null|           20240925|        null|\n|    X115526|   4511|          null|           20240925|        null|\n|    X113534|   4521|          null|           20240925|        null|\n|    X115002|   4988|          null|           20240925|        null|\n|    X119378|   4359|          null|           20240925|        null|\n|    X111731|   4720|          null|           20240925|        null|\n|    X119863|   4844|          null|           20240925|        null|\n|    X119414|   4395|          null|           20240925|        null|\n|    X114188|   4174|          null|           20240925|        null|\n+-----------+-------+--------------+-------------------+------------+\nonly showing top 20 rows\n\n/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Visualize data with matplotlib\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "from botocore.exceptions import ClientError\ntry:\n    glueContext.write_dynamic_frame_from_options(\n                    frame = dyf,\n                    connection_type=\"dynamodb\",\n                    connection_options={\"dynamodb.output.tableName\": \"employee_department_interactions\", \"dynamodb.throughput.write.percent\":\"1.0\"}\n                    )\nexcept Exception as e:\n    error_message = str(e)\n    print(\"error message is \", error_message, \"could you spot me\" )\n    if \"ResourceNotFoundException\" in error_message:\n        print(\"Lets create the table\")\n\n    # if True:#e.response['Error']['Code'] == 'ResourceNotFoundException':\n    #                 logger.error(f\"Table {dynamodb_table} does not exist. Creating it with default PK SK values\")\n    #                 create_dynamodb_table(dynamodb_table, logger)\n    #                 synthesize(glueContext,\n    #                            spark,\n    #                            config_dict,\n    #                            dynamodb_table,\n    #                            keyfilepath,\n    #                            logger,\n    #                            execmode)\n    # else:\n    #     logger.error(f\"Unexpected error: {e}\")\n    #     raise         ",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "error message is  An error occurred while calling o136.pyWriteDynamicFrame.\n: com.amazonaws.services.dynamodbv2.model.ResourceNotFoundException: Requested resource not found: Table: employee_department_interactions not found (Service: AmazonDynamoDBv2; Status Code: 400; Error Code: ResourceNotFoundException; Request ID: IB38I1ED9PH9TU89AEJU5Q5673VV4KQNSO5AEMVJF66Q9ASUAAJG; Proxy: null)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n\tat com.amazonaws.services.dynamodbv2.AmazonDynamoDBClient.doInvoke(AmazonDynamoDBClient.java:6409)\n\tat com.amazonaws.services.dynamodbv2.AmazonDynamoDBClient.invoke(AmazonDynamoDBClient.java:6376)\n\tat com.amazonaws.services.dynamodbv2.AmazonDynamoDBClient.executeDescribeTable(AmazonDynamoDBClient.java:2332)\n\tat com.amazonaws.services.dynamodbv2.AmazonDynamoDBClient.describeTable(AmazonDynamoDBClient.java:2296)\n\tat com.amazonaws.services.dynamodbv2.AmazonDynamoDBClient.describeTable(AmazonDynamoDBClient.java:2344)\n\tat com.amazonaws.services.glue.iopsmanagement.WriteIopsCalculator.calculateTargetIops(WriteIopsCalculator.scala:46)\n\tat com.amazonaws.services.glue.iopsmanagement.WriteIopsCalculator.<init>(WriteIopsCalculator.scala:34)\n\tat com.amazonaws.services.glue.DynamoDbDataSink.writeDynamicFrame(DataSink.scala:902)\n\tat com.amazonaws.services.glue.DataSink.pyWriteDynamicFrame(DataSink.scala:72)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n could you spot me\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Write the data in the DynamicFrame to a location in Amazon S3 and a table for it in the AWS Glue Data Catalog\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "s3output = glueContext.getSink(\n  path=\"s3://bucket_name/folder_name\",\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  partitionKeys=[],\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\ns3output.setCatalogInfo(\n  catalogDatabase=\"demo\", catalogTableName=\"populations\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(DyF)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		}
	]
}